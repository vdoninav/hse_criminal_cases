{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Загрузка и подготовка данных\n",
    "data = []\n",
    "with open('105_first.json', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            # Преобразование каждой строки в JSON объект\n",
    "            json_obj = json.loads(line)\n",
    "            if json_obj[\"text\"] is None:\n",
    "                continue\n",
    "            data.append(json_obj)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Ошибка чтения\")\n",
    "            continue\n",
    "\n",
    "# Создание DataFrame из списка JSON объектов\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Объединяем все категории в один столбец для упрощения (пример)\n",
    "df['category'] = df[['region', 'instance', 'name', 'caseNumber', 'entryDate', 'names', 'judge', 'resultDate', 'decision', 'endDate']].apply(lambda x: ','.join(x.dropna().astype(str)), axis=1)\n",
    "\n",
    "# Токенизация текста\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenized_data = tokenizer(df['text'].tolist(), padding=True, truncation=True, max_length=65536, return_tensors=\"pt\")\n",
    "\n",
    "texts = df['text'].tolist()  # Список текстов\n",
    "le = LabelEncoder()\n",
    "df['labels'] = le.fit_transform(df['category'])\n",
    "labels = df['labels'].tolist()\n",
    "\n",
    "# Токенизация текстов\n",
    "tokenized_texts = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Преобразование тензоров в списки для совместимости с train_test_split (если необходимо)\n",
    "input_ids = tokenized_texts['input_ids'].tolist()  # Пример для PyTorch\n",
    "\n",
    "# Теперь размерности должны совпадать\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(input_ids, labels, test_size=0.1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T16:35:54.440184Z",
     "start_time": "2024-01-23T16:35:08.158779Z"
    }
   },
   "id": "ae61f6c55856bb3b"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "vars() argument must have __dict__ attribute",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 19\u001B[0m\n\u001B[1;32m      3\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[1;32m      4\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./results\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      5\u001B[0m     num_train_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m     logging_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./logs\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     10\u001B[0m )\n\u001B[1;32m     12\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m     13\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m     14\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[1;32m     15\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39mtrain_texts,  \u001B[38;5;66;03m# Здесь необходимо подготовить данные в формате, пригодном для Trainer\u001B[39;00m\n\u001B[1;32m     16\u001B[0m     eval_dataset\u001B[38;5;241m=\u001B[39mtest_texts     \u001B[38;5;66;03m# Аналогично, подготовьте тестовые данные\u001B[39;00m\n\u001B[1;32m     17\u001B[0m )\n\u001B[0;32m---> 19\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages/transformers/trainer.py:1555\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1553\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   1554\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1555\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inner_training_loop(\n\u001B[1;32m   1556\u001B[0m         args\u001B[38;5;241m=\u001B[39margs,\n\u001B[1;32m   1557\u001B[0m         resume_from_checkpoint\u001B[38;5;241m=\u001B[39mresume_from_checkpoint,\n\u001B[1;32m   1558\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[1;32m   1559\u001B[0m         ignore_keys_for_eval\u001B[38;5;241m=\u001B[39mignore_keys_for_eval,\n\u001B[1;32m   1560\u001B[0m     )\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages/transformers/trainer.py:1815\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   1812\u001B[0m     rng_to_sync \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1814\u001B[0m step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m-> 1815\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, inputs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(epoch_iterator):\n\u001B[1;32m   1816\u001B[0m     total_batched_samples \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1817\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m rng_to_sync:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages/accelerate/data_loader.py:451\u001B[0m, in \u001B[0;36mDataLoaderShard.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    449\u001B[0m \u001B[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001B[39;00m\n\u001B[1;32m    450\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 451\u001B[0m     current_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(dataloader_iter)\n\u001B[1;32m    452\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    453\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    672\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    673\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 674\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    675\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    676\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcollate_fn(data)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages/transformers/trainer_utils.py:707\u001B[0m, in \u001B[0;36mRemoveColumnsCollator.__call__\u001B[0;34m(self, features)\u001B[0m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, features: List[\u001B[38;5;28mdict\u001B[39m]):\n\u001B[1;32m    706\u001B[0m     features \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_remove_columns(feature) \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m features]\n\u001B[0;32m--> 707\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_collator(features)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages/transformers/data/data_collator.py:70\u001B[0m, in \u001B[0;36mdefault_data_collator\u001B[0;34m(features, return_tensors)\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001B[39;00m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;66;03m# have the same attributes.\u001B[39;00m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001B[39;00m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;66;03m# on the whole batch.\u001B[39;00m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_tensors \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch_default_data_collator(features)\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m return_tensors \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tf_default_data_collator(features)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages/transformers/data/data_collator.py:109\u001B[0m, in \u001B[0;36mtorch_default_data_collator\u001B[0;34m(features)\u001B[0m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(features[\u001B[38;5;241m0\u001B[39m], Mapping):\n\u001B[0;32m--> 109\u001B[0m     features \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mvars\u001B[39m(f) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m features]\n\u001B[1;32m    110\u001B[0m first \u001B[38;5;241m=\u001B[39m features[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    111\u001B[0m batch \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages/transformers/data/data_collator.py:109\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(features[\u001B[38;5;241m0\u001B[39m], Mapping):\n\u001B[0;32m--> 109\u001B[0m     features \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mvars\u001B[39m(f) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m features]\n\u001B[1;32m    110\u001B[0m first \u001B[38;5;241m=\u001B[39m features[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    111\u001B[0m batch \u001B[38;5;241m=\u001B[39m {}\n",
      "\u001B[0;31mTypeError\u001B[0m: vars() argument must have __dict__ attribute"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(le.classes_))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_texts,  # Здесь необходимо подготовить данные в формате, пригодном для Trainer\n",
    "    eval_dataset=test_texts     # Аналогично, подготовьте тестовые данные\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T16:36:05.409009Z",
     "start_time": "2024-01-23T16:36:02.132708Z"
    }
   },
   "id": "891a863d88563426"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "771c6224ec9cf8c1"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "0       ПРИГОВОР\\nИМЕНЕМ РОССИЙСКОЙ ФЕДЕРАЦИИ\\nс. Кыре...\n1       ПРИГОВОР\\nИМЕНЕМ РОССИЙСКОЙ ФЕДЕРАЦИИ\\nс. Кыре...\n2                                                    None\n3       Дело №\\nПРИГОВОР\\nИМЕНЕМ РОССИЙСКОЙ ФЕДЕРАЦИИ\\...\n4       ПРИГОВОР\\nИМЕНЕМ РОССИЙСКОЙ ФЕДЕРАЦИИ\\nс. Кыре...\n                              ...                        \n2850    Уголовное дело № 1-629/2016\\nП Р И Г О В О Р\\n...\n2851                                                 None\n2852    \\nУголовное дело №1-453-2012\\nП Р И Г О В О Р\\...\n2853    Уголовное дело № 1-715/2011\\nП Р И Г О В О Р\\n...\n2854    Уголовное дело № 1-600/ 2013\\n\\nП Р И Г О В О ...\nName: text, Length: 2855, dtype: object"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T16:22:42.384491Z",
     "start_time": "2024-01-23T16:22:42.304778Z"
    }
   },
   "id": "83a93a073d37a2c3"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[  101,   524, 18005,  ...,     0,     0,     0],\n        [  101,   524, 18005,  ...,     0,     0,     0],\n        [  101, 52935, 11602,  ...,     0,     0,     0],\n        ...,\n        [  101,   528, 85952,  ...,     0,     0,     0],\n        [  101,   528, 85952,  ...,     0,     0,     0],\n        [  101,   528, 85952,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        ...,\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]])}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T16:28:25.032796Z",
     "start_time": "2024-01-23T16:28:25.005644Z"
    }
   },
   "id": "3bc831607cd29d7c"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Допустим, у вас есть DataFrame с текстами и метками\n",
    "texts = df['text'].tolist()  # Список текстов\n",
    "le = LabelEncoder()\n",
    "df['labels'] = le.fit_transform(df['category'])\n",
    "labels = df['labels'].tolist()\n",
    "\n",
    "# Токенизация текстов\n",
    "tokenized_texts = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Преобразование тензоров в списки для совместимости с train_test_split (если необходимо)\n",
    "input_ids = tokenized_texts['input_ids'].tolist()  # Пример для PyTorch\n",
    "\n",
    "# Теперь размерности должны совпадать\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(input_ids, labels, test_size=0.1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T16:30:41.928907Z",
     "start_time": "2024-01-23T16:30:34.055015Z"
    }
   },
   "id": "15dd575be758115a"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting accelerate\r\n",
      "  Downloading accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/vdav/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages (from accelerate) (1.26.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vdav/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages (from accelerate) (23.1)\r\n",
      "Requirement already satisfied: psutil in /Users/vdav/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages (from accelerate) (5.9.0)\r\n",
      "Requirement already satisfied: pyyaml in /Users/vdav/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages (from accelerate) (6.0.1)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/vdav/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages (from accelerate) (2.1.2)\r\n",
      "Requirement already satisfied: huggingface-hub in /Users/vdav/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages (from accelerate) (0.17.3)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/vdav/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages (from accelerate) (0.4.0)\r\n",
      "Requirement already satisfied: filelock in /Users/vdav/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/vdav/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.7.1)\r\n",
      "Requirement already satisfied: sympy in /Users/vdav/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/vdav/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/vdav/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /Users/vdav/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2023.12.2)\r\n",
      "Requirement already satisfied: requests in /Users/vdav/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/vdav/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.65.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/vdav/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vdav/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vdav/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vdav/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vdav/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/vdav/opt/anaconda3/envs/python3_11_5/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n",
      "Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m270.9/270.9 kB\u001B[0m \u001B[31m1.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: accelerate\r\n",
      "Successfully installed accelerate-0.26.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T16:33:20.741428Z",
     "start_time": "2024-01-23T16:33:18.049083Z"
    }
   },
   "id": "240a5dd0524574e0"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "zsh:1: no matches found: transformers[torch]\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-23T16:33:50.475528Z",
     "start_time": "2024-01-23T16:33:50.307738Z"
    }
   },
   "id": "69d2acbf803a4af6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2cad4a4f45db9688"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
