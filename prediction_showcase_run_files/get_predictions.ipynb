{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-06 06:30:33.966595: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Extracting labels: 100%|██████████| 73734/73734 [22:28<00:00, 54.68it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n",
      "(73734, 2)\n",
      "(256029, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting labels: 100%|██████████| 10000/10000 [01:12<00:00, 137.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n",
      "(10000, 2)\n",
      "(38285, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting labels: 100%|██████████| 13934/13934 [01:30<00:00, 154.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n",
      "(13934, 2)\n",
      "(53022, 2)\n"
     ]
    }
   ],
   "source": [
    "with open(\"preprocessing.py\") as f:\n",
    "    exec(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers \n",
    "\n",
    "import params\n",
    "from prepare_data import TokenLabelDataset, csv_preprocessing\n",
    "from compute_metrics import compute_metrics\n",
    "\n",
    "print(params.DEVICE)\n",
    "\n",
    "model = transformers.AutoModelForTokenClassification.from_pretrained(params.MODEL_CKECKPOINT, num_labels=len(params.LABEL_LIST))\n",
    "model.config.id2label = dict(enumerate(params.MODEL_CKECKPOINT))\n",
    "model.config.label2id = {v: k for k, v in model.config.id2label.items()}\n",
    "model = model.to(params.DEVICE)\n",
    "\n",
    "tokenizer = params.TOKENIZER\n",
    "\n",
    "num_epochs = 0\n",
    "\n",
    "args = transformers.TrainingArguments(\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    output_dir=params.CHECKPOINTS_DIR,\n",
    "    learning_rate = params.LR,\n",
    "    per_device_train_batch_size = params.BATCH_SIZE,\n",
    "    per_device_eval_batch_size = params.BATCH_SIZE,\n",
    "    num_train_epochs = num_epochs, # epoch count !!!\n",
    "    weight_decay = params.WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "data_collator = transformers.DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset = TokenLabelDataset(params.SAVE_DIR + 'validation.pkl'),\n",
    "    eval_dataset = TokenLabelDataset(params.SAVE_DIR + 'validation.pkl'),\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "print(trainer.args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TokenLabelDataset(csv_preprocessing('../small_test.csv', '../small_test.pkl'))\n",
    "predictions, labels, _ = trainer.predict(test_dataset)\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "true_predictions = [\n",
    "    [params.LABEL_LIST[p] for (p, l) in zip(prediction, label)]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_pickle('../small_test.pkl')\n",
    "\n",
    "tokenized_words = [params.TOKENIZER.convert_ids_to_tokens(params.TOKENIZER(test_df.loc[i, \"words\"], \n",
    "                                                                           is_split_into_words=True,\n",
    "                                                                           max_length=512, truncation=True,\n",
    "                                                                           padding='max_length',)[\"input_ids\"]) for i in range(len(test_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = [params.TOKENIZER(test_df.loc[i, \"words\"], is_split_into_words=True, max_length=512, padding='max_length', truncation=True,).word_ids() \n",
    "          for i in range(len(test_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мировой: B-IND\n",
      "судья: I-IND\n",
      "правонарушение: B-CR\n",
      "ответственность: B-CR\n",
      "личность: B-IND\n",
      "\n",
      "\n",
      "Мировой: B-IND\n",
      "судья: I-IND\n",
      "водитель: B-IND\n",
      "АО: B-LE\n",
      "\n",
      "АО: B-LE\n",
      "освидетельствование: B-LAW\n",
      "\n",
      "арест: B-LAW\n",
      "правонарушение: B-CR\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for j in range(len(test_df.loc[i, 'words'])):\n",
    "        if test_df.loc[i, 'labels'][j] in params.LABEL_LIST[:-1]:\n",
    "            print(test_df.loc[i, 'words'][j]+':' , test_df.loc[i, 'labels'][j])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мировой -> Мир: B-IND\n",
      "Мировой -> ##овой: B-IND\n",
      "судья -> суд: I-IND\n",
      "судья -> ##ья: I-IND\n",
      "правонарушение -> право: B-CR\n",
      "правонарушение -> ##нар: B-CR\n",
      "правонарушение -> ##уш: B-CR\n",
      "правонарушение -> ##ение: B-CR\n",
      "ответственность -> ответ: B-CR\n",
      "ответственность -> ##ственность: B-CR\n",
      "личность -> лично: B-IND\n",
      "личность -> ##сть: B-IND\n",
      "\n",
      "Мировой -> Мир: B-IND\n",
      "Мировой -> ##овой: B-IND\n",
      "судья -> суд: I-IND\n",
      "судья -> ##ья: I-IND\n",
      "\n",
      "Мировой -> Мир: B-IND\n",
      "Мировой -> ##овой: B-IND\n",
      "судья -> суд: I-IND\n",
      "судья -> ##ья: I-IND\n",
      "водитель -> води: B-IND\n",
      "водитель -> ##тель: B-IND\n",
      "\n",
      "освидетельствование -> ##ство: B-LAW\n",
      "АО -> АО: B-LE\n",
      "\n",
      "арест -> ар: B-LAW\n",
      "арест -> ##ест: B-LAW\n",
      "арест -> ар: B-LAW\n",
      "арест -> ##ест: B-LAW\n",
      "правонарушение -> право: B-CR\n",
      "правонарушение -> ##нар: B-CR\n",
      "правонарушение -> ##уш: B-CR\n",
      "правонарушение -> ##ение: B-CR\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    # print('!!!!!!!', len(word_ids[i]), len(true_predictions[i], ))\n",
    "    for j in range(len(true_predictions[i])):\n",
    "        if true_predictions[i][j] in params.LABEL_LIST[:-1]:\n",
    "            print(test_df.loc[i, 'words'][word_ids[i][j]] , '->' , tokenized_words[i][j]+':' , true_predictions[i][j])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
